# rpc
rpc by go

[gRPC原理学习概述](./gRPC学习.md)


## RPC原理解析

### 简介：
RPC 的全称是 Remote Procedure Call，即远程过程调用

具有以下作用：
1. 屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法；
2. 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑。

TIPs:

1. 使用rpc的场景是否合适，
2. 什么是否需要开启压缩，根据配置，根据部署机器配置，根据网络环境，根据传输数据大小
3. 调用过程超时处理，以及失败重试机制，例如dubbo的failfast，failover等
4. 服务集群注意点
    1. 服务注册，发现，服务注册中心
    2. 服务治理，服务分组，服务别名，服务限流，服务降级，服务调用链，链路跟踪
    3. 服务监控，调用链监控，方法监控，数据指标监控（TPS，调用量，可用率，调用返回时间，服务网络响应时间）
    4. 服务日志，聚合查询，整理，告警
    5. 服务集群化，分组化的在线配置中心。支持日志等级控制，服务控制

### RPC通信流程：
步骤如下：

1. RPC是远程调用，需要网络传输数据，并且由于常用于业务系统之间进行远程调用，所以需要使用TCP来进行传输

2. 网络传输的数据必须是二进制数据，但是调用方请求的出入参数都是对象，所以需要使用可逆的算法，来将对象转化为二进制数据，这一步叫做序列化

3. 调用方持续的将请求序列化为二进制数据，经过TCP后传输给了服务提供方。服务提供方如何知道请求的数据的大小，以及请求的是哪个接口类型；因此需要约定数据包的格式，这个步骤就是协议的约定

4. 根据协议格式，服务提供者可以正确的从二进制数据中分割出不同的请求，同事根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象，这一步就叫反序列化

5. 服务提供方根据反序列化出来的请求对象，找到对象的实现类，完成方法调用

6. 将执行结果序列化后，回写到TCP通道中。调用方获取到应答数据后，再进行反序列化得到Reponse数据，完成RPC调用

7. 简化调用链，利用反射或者其他方法让调用方在调用远程方法时，能够像调用本地接口一样

    ![image-20210522164901880](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522164901880.png)

### RPC协议

**RPC协议简介**

- RPC请求在发送到网络中之前，需要将请求转为二进制数据，基于TCP连接和服务方通信，TCP链接会根据系统配置和TCP窗口大小，在同一个TCP链接中，对数据包进行拆分，合并。服务方需要正确处理TCP通道中的二进制数据。

- RPC协议是一种应用层协议，主要负责应用间的通信，相对于HTTP协议，需要的性能更高，并且RPC是有状态的协议，请求和响应一一对应。RPC一般会设计更加紧凑的私有协议

**RPC协议的设计**

- 消息边界语义：利用一个定长数据来保存整个请求协议体的大小；先读取固定长度的位置里面的值，得到协议体长度，再去读取整个协议体的数据

    ![image-20210522162421147](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522162421147.png)

- 协议数据序列化方法信息：利用定长的位置存储协议数据的序列化方式

- 将整个协议分为协议头和协议体，得到定长协议头，该协议头是不可扩展的

    ![image-20210522162430292](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522162430292.png)

- 可扩展协议，将协议头改为可扩展的。将协议分为三部分：固定部分，协议头内容，协议体内容；前两部分统称为协议头

    ![image-20210522162827231](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522162827231.png)

- RPC为了吞吐量，都是异步并发发送的请求，等待服务应答，因此需要消息ID，来判断应答对应哪个请求




**golang 实现rpc序列化**

RPC客户端调用如下:
`err = client.Call("service.Method",args,&reply)`
客户端发送的请求有包含服务名,方法名,参数列表
服务端返回的响应有错误,返回值
将请求和响应中的参数和返回值抽象为body,那么剩余的信息可以抽象为一个Header

```go
type Header struct {
	ServiceMethod string //服务名和方法名
	Seq uint64 //请求序号
	Error string //客户端为空,服务端如果发生错误,会把错误信息放到Error中
}
```

### RPC网络通信

**常见的网络IO模型**

- 同步阻塞 IO（BIO）
    - 在 Linux 中，默认情况下所有的 socket 都是 blocking 的
    - 应用进程发起 IO 系统调用后，应用进程被阻塞，转到内核空间处理。之后，内核开始等待数据，等待到数据之后，再将内核中的数据拷贝到用户内存中，整个 IO 处理完毕后返回进程。最后应用的进程解除阻塞状态，运行业务逻辑。
    - ![image-20210522185621450](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522185621450.png)
    - 系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。而在这两个阶段中，应用进程中 IO 操作的线程会一直都处于阻塞状态，如果是基于 Java 多线程开发，那么每一个 IO 操作都要占用线程，直至 IO 操作结束。
    - 阻塞 IO 每处理一个 socket 的 IO 请求都会阻塞进程（线程），但使用难度较低。在并发量较低、业务逻辑只需要同步进行 IO 操作的场景下，阻塞 IO 已经满足了需求，并且不需要发起 select 调用，开销上还要比 IO 多路复用低。
- 同步非阻塞 IO（NIO）
- 同步IO 多路复用（select，poll，epoll）
    - 多路复用 IO 是在高并发场景中使用最为广泛的一种 IO 模型
    - linux总的多个网络连接的 IO 可以注册到一个复用器（select）上，当用户进程调用了 select，那么整个进程会被阻塞。同时，内核会“监视”所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从内核中拷贝到用户进程。
    - 优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求。用户可以注册多个 socket，然后不断地调用 select 读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。
    - IO 多路复用更适合高并发的场景，可以用较少的进程（线程）处理较多的 socket 的 IO 请求。
- 异步非阻塞 IO（AIO）

**RPC网络io模型**

RPC 调用在大多数的情况下，是一个高并发调用的场景

- 在 RPC 框架的实现中，在网络通信的处理上，我们会选择 IO 多路复用的方式。

- 选择基于 Reactor 模式实现的io框架来实现IO多路复用

- 在 Linux 环境下，也要开启 epoll 来提升系统性能（Windows 环境下是无法开启 epoll 的，因为系统内核不支持）。

**网络io中的零拷贝**

系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。

- 等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中
- 拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中。

应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA 将这份数据拷贝到网卡中，最后由网卡发送出去。一次写操作数据要拷贝两次才能通过网卡发送出去

![image-20210522192234064](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522192234064.png)

- 零拷贝技术
    - 零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作，应用进程每一次的读写操作，都可以通过一种方式，让应用进程向用户空间写入或者读取数据，就如同直接向内核空间写入或者读取数据一样，再通过 DMA 将内核中的数据拷贝到网卡，或将网卡中的数据 copy 到内核。
- 零拷贝实现
    - mmap+write 方式，核心原理是通过虚拟内存来解决的
    - sendfile 方式
- Netty零拷贝实现：
    - 用户空间数据操作零拷贝优化
        - 收到数据包后，在对数据包进行处理时，需要根据协议，处理数据包，在进行处理时，免不了需要进行在用户空间内部内存中进行拷贝处理，Netty就是在用户空间中对数据操作进行优化
        - Netty 提供了 CompositeByteBuf 类，它可以将多个 ByteBuf 合并为一个逻辑上的  ByteBuf，避免了各个 ByteBuf 之间的拷贝。
        - ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝。
        - 通过 wrap 操作，我们可以将 byte[] 数组、ByteBuf、ByteBuffer  等包装成一个 Netty ByteBuf 对象, 进而避免拷贝操作。
    - 用户空间与内核空间之间零拷贝优化
        - Netty  的  ByteBuffer 可以采用 Direct Buffers，使用堆外直接内存进行 Socket 的读写操作，效果和虚拟内存所实现的效果是一样的。
        - Netty  还提供  FileRegion  中包装  NIO  的  FileChannel.transferTo()  方法实现了零拷贝，这与 Linux  中的  sendfile  方式在原理上也是一样的。

### RPC框架设计：

#### 屏蔽处理流程

- java使用动态代理屏蔽实现细节
- golang使用反射等，来实现的

#### RPC架构

##### 网络传输模块

用于收发二进制数据

##### 协议模块

保证数据在网络中正确传输，包括序列化和反序列化功能，数据压缩功能，以及通信协议约定

##### Bootstrap模块

用于屏蔽RPC细节，利用反射或者代理让远程调用大大简化

##### 服务治理模块

赋予RPC服务集群能力，包括服务注册和发现，负载均衡，连接管理，路由，容错和配置管理

架构图如下：

![](https://gitee.com/Euraxluo/images/raw/master/picgo/30f52b433aa5f103114a8420c6f829fb.jpg)

##### 利用微内核架构，将组件插件化

将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离并提供接口的默认实现。

提升了RPC框架的可扩展性，实现了开闭原则，用户可以非常方便地通过插件扩展实现自己的功能，而且不需要修改核心功能的本身；其次保持了核心包的精简，依赖外部包少，这样可以有效减少开发人员引入 RPC 导致的包版本冲突问题。

![](https://gitee.com/Euraxluo/images/raw/master/picgo/a3688580dccd3053fac8c0178cef4ba6.jpg)



### 服务发现：

#### 概述：

服务发现（Service Discoery）要解决的是分布式系统中最常见的问题之一，即在同一个分布式系统中的进程如何才能找到对方并建立连接。

##### 服务发现组件的需求

服务发现组件需要以下一些功能：

- 根据服务名的得到服务可用列表
- 服务注册功能，因此高组件是一个独立的，简单的第三方存储，并且存储极简化。
- 同时服务发现组件还需要有服务探活功能，应当提供很多的服务探活选项

##### 服务代理

服务发现组件的从需求上看是服务代理，常见对的服务代理有：

1. 网络代理
    - 如果是使用http通信，那么可以使用nginx作为反向代理，转到各个服务
    - 如果是RPC服务则可以使用LVS或者ESB之内的网络代理服务地址
    - 缺点：当服务增多是，需要维护超多的网络代理，最后将陷入到运维灾难中
2. DNS方式
    - 为服务A配置域名，然后通过配置两个分别指向服务A的实例，客户端只需要使用配置A的域名就可以
    - 问题：DNS是IP级别，无法处理端口信息，DNS携带的数据较少，节点权重，序列化信息等数据无法传递。

服务代理无法满足服务发现组件对的所有需求，所以需要找另外的组件。

#### zookeeper做为服务发现的问题

Zookeeper旨在解决大规模分布式应用场景下的服务协调同步问题；他可以为同在一个分布式系统中的其他服务提供：统一命名服务，配置管理服务，分布式锁服务，集群管理服务等。

##### CAP（C-数据一致性；A-服务可用性；P-服务对网络分区故障的容错性）

zk是一个CP的，即咋子任何时候对于ZK的访问请求都能得到一致的是数据结果，同时系统对于网络分割具备容错性。

##### ZK解决的问题

Zk是一个分布式协调服务，他被设计用于保证数据在其管辖下，在所有服务之前保持同步，一致，因此ZK被设计为CP的。

##### ZK作为服务发现服务的问题

由于zk不能保证每次服务的可用性：

1. 因为对于服务发现服务来说，宁可返回某个包含了不实信息的结果也比什么都不返回的好。
2. 宁可返回某服务5分钟之前在在某几台服务器上可用的信息，也不能因为暂时的网络故障找不到可用的服务器。
3. ZK中，若某网络分区中的节点数小于ZK选取leader节点的法定人数，那么这些节点将会断开，就无法正确提供服务了
4. ZK的特点是强一致性，所有导致ZK集群的每个节点数据在发生更新时，需要通知其他ZK节点同时执行更新，所以当大量服务节点上线时，可能会导致ZK集群无法承载

##### 局限性:

1. 网络化分后，强一致性导致服务注册机制会失效

    ZAB协议保证数据一致性，当发生网络分割时，会破坏服务的整体联通性

2. 持久化存储和事务日志

    为了保证数据一致性，zk使用的事务日志，当集群半数节点写入成功时，该事务有效。同时事务写使用的2PC提交的方式

    但是注册中心只关心实时的健康服务列表，因为调用方不关心历史服务和状态

3. 服务探活

    ZK注册镇中心通常利用session活性心跳和临时节点机制进行服务探活

    将服务的健康检查检测绑定在了ZK对于Session的健康监测上。然后其实应该由服务方决定探活方式

4. 服务容灾

    服务调用链路弱依赖注册中心，同时ZK客户端并无客户端缓存机制

##### 改良

1. 加上服务可用性。使用客户端缓存，当部分节点与zk断开时，每个节点依然能从本地缓存中获取到数据，但是ZK不能保证所有节点任何时刻都能缓存所有的服务注册信息。
2. 将ZK的强一致性改为Ap并保证最终一致性：当我们需要最终一致性时，可以使用消息总线机制。注册数据可以全量缓存在每个注册中心内存中。通过消息总线同步数据。当有一个节点接收到服务节点注册时，会产生一个消息推送到消息总线，最后再通过消息总线通知给其他的注册中心节点更新数据，并进行服务下发，从而达到注册中心数据的最终一致性。

#### Eureka:专为服务发现设计对的开源组件

Eureka由Eureka服务器和Eureka客户端组成，Eureka服务器作为服务注册服务器，Eureka客户端是一个java客户端，用于简化与服务器的操作，作为轮询负载均衡器，并提供服务的故障切换支持。

##### Eureka Server：注册中心服务端

注册中心服务端主要提供了三个功能

**服务注册**

服务提供者启动后，会通过Eureka Client 向Eureka Server注册信息，Eureka Server会存储该服务的信息，Eureka Server内部有二层缓存机制来维护整个注册表

**提供注册表**

服务消费者在调用服务时，如果Eureka Client 没有缓存注册表的话，会从Eureka Server 获取最新的注册表

**同步状态**

Eureka Client 通过注册，心跳机制和Eureka Server 同步当前客户端的状态

##### Eureka Client：注册中心客户端

Eureka Client是一个java客户端，用于简化与Eureka Server的交互，Eureka Client会拉取，更新和缓存Eureka Server中的信息。因此当所有的Eureka Server节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者，但是当服务更改时会出现信息不一致

**Registry：服务注册**

服务的提供者，将自身注册到注册中心，服务提供者也是一个Eureka Client。当Eureka Client向Eureka Server注册时，它提供自身的元数据。

**Renew：服务续约**

Eureka Client会每间隔30s发送一次心跳进行续约。如果续约来告知Eureka Server该Eureka Client运行正常，没有正常问题。默认情况下，如果Eureka Server在90s内没有收到Eureka Client的续约，Server端就会将实例从注册表中删除。

**Eviction：服务剔除**

当Eureka Client和Eureka Server不在有心跳时，Eureka Server会从该服务实例从服务注册列表中删除，即服务剔除

**Cancel：服务下线**

Eureka Client在程序关闭时向Eureka Server发送取消请求。发送请求后，该客户端实例信息将从Eureka Server的实力注册表中删除。该下线请求不会自动完成，需要调用特殊的方法

**GetRegistry：获取注册列表信息**

Eureka Client从服务器获取注册表信息，并将其缓存在本地。客户端会使用该信息查找其他服务，从而进行远程调用。该注册列表信息会定期清理一次。重新拉取新的注册表信息

**Remote Call：远程调用**

当Eureka Client从注册中心获取到1服务提供者信息后，就可以通过Http请求调用对应的服务了；服务提供者有多个时，Eureka Client客户端会通过Ribbon进行自动负载均衡。


##### 高可用：

在Eureka平台中，若某台服务器宕机，Eureka服务不会有类似Zookeeper选举的过程，客户端会自动切换到新的Eureka节点；当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理中；因此不用担心会有服务器从集群中剔除的风险

##### 应对网络分割故障

当网络分割故障出现时，每个Eureka节点会持续的对外服务，接收新的服务注册请求同时将他们提供给下游的服务发现请求。这样在一个子网中，新发布的服务依然可以被发现与访问

##### 节点自我保护

Eureka内置了心跳服务，用于淘汰一些假死的服务器；如果在Eureka中注册的服务，心跳变的迟缓，Eureka会将其整个剔除出管理范围。这个功能在发生网路分割故障时会很危险。因为可能服务器是正常的，只不过是因为网络问题到了一个子网中

Netflix考虑添加了自我保护机制,如果Eureka服务节点在短时间内丢失了大量心跳连接，那么该服务节点会进入自我保护状态，这些节点的服务注册信息将不会过期，即便是假死状态，以防还有客户端会向该假死节点发起请求。同时当Eureka节点恢复后，会退出自我保护模式

##### 客户端缓存

Eureka最后还有客户端缓存的功能。当所有的Eureka集群节点都失效，或者发生网络分割故障导致客户端不能访问任何一台Eureka服务器。Eureka消费者依然可以通过客户端缓存找到现有对的服务注册信息



#### etcd 工作原理

##### 保证一致性：

etcd 使用raft协议来维护集群内各个节点状态的一致性。etcd集群是一个分布式系统，由多个节点相互通信构成一个整体对外服务，每个节点都存储了完整的数据，并且通过Rat协议保证每个节点维护的数据是一致的。

每个etcd节点都维护了一个状态机，并且任意时刻至多存在一个有效的主节点，主节点处理所有来自客户端的写操作，通过raft协议保证写操作对状态机的改动会可靠的同步到其他节点

## 实现篇





客户端与服务端通信需要协商内容,对于rpc来说,会在报文的最开始使用固定的直接协商信息,包括序列化方式,压缩方式,header长度,body长度等
对于我们的rpc来说,目前需要协商的内容是编解码方式.我们可以使用选项模式,来应对以后的变化

```go
const MagicNumber = 0x3bef5c
type Option struct {
	MagicNumber int //魔数,标识服务端的系统
	CodecType codec.Type
}

var DefaultOption = &Option{
	MagicNumber:MagicNumber,
	CodecType:codec.GobType,
}
```

我们的客户端为了便于实现,
固定采用 JSON 编码 Option，后续的 header 和 body 的编码方式由 Option 中的 CodeType 指定，
服务端首先使用 JSON 解码 Option，然后通过 Option 中指定的 CodeType 解码剩余的内容。即报文将以这样的形式发送
| Option{MagicNumber: xxx, CodecType: xxx} | Header{ServiceMethod ...} | Body interface{} |
| <------      固定 JSON 编码      ------>  | <-------   编码方式由 CodeType 决定   ------->|

### 