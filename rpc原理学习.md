## RPC原理解析

### 简介：
RPC 的全称是 Remote Procedure Call，即远程过程调用

具有以下作用：
1. 屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法；
2. 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑。

TIPs:

1. 使用rpc的场景是否合适，
2. 什么是否需要开启压缩，根据配置，根据部署机器配置，根据网络环境，根据传输数据大小
3. 调用过程超时处理，以及失败重试机制，例如dubbo的failfast，failover等
4. 服务集群注意点
    1. 服务注册，发现，服务注册中心
    2. 服务治理，服务分组，服务别名，服务限流，服务降级，服务调用链，链路跟踪
    3. 服务监控，调用链监控，方法监控，数据指标监控（TPS，调用量，可用率，调用返回时间，服务网络响应时间）
    4. 服务日志，聚合查询，整理，告警
    5. 服务集群化，分组化的在线配置中心。支持日志等级控制，服务控制

### RPC通信流程：
步骤如下：

1. RPC是远程调用，需要网络传输数据，并且由于常用于业务系统之间进行远程调用，所以需要使用TCP来进行传输

2. 网络传输的数据必须是二进制数据，但是调用方请求的出入参数都是对象，所以需要使用可逆的算法，来将对象转化为二进制数据，这一步叫做序列化

3. 调用方持续的将请求序列化为二进制数据，经过TCP后传输给了服务提供方。服务提供方如何知道请求的数据的大小，以及请求的是哪个接口类型；因此需要约定数据包的格式，这个步骤就是协议的约定

4. 根据协议格式，服务提供者可以正确的从二进制数据中分割出不同的请求，同事根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象，这一步就叫反序列化

5. 服务提供方根据反序列化出来的请求对象，找到对象的实现类，完成方法调用

6. 将执行结果序列化后，回写到TCP通道中。调用方获取到应答数据后，再进行反序列化得到Reponse数据，完成RPC调用

7. 简化调用链，利用反射或者其他方法让调用方在调用远程方法时，能够像调用本地接口一样

    ![image-20210522164901880](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522164901880.png)

### RPC协议

**RPC协议简介**

- RPC请求在发送到网络中之前，需要将请求转为二进制数据，基于TCP连接和服务方通信，TCP链接会根据系统配置和TCP窗口大小，在同一个TCP链接中，对数据包进行拆分，合并。服务方需要正确处理TCP通道中的二进制数据。

- RPC协议是一种应用层协议，主要负责应用间的通信，相对于HTTP协议，需要的性能更高，并且RPC是有状态的协议，请求和响应一一对应。RPC一般会设计更加紧凑的私有协议

**RPC协议的设计**

- 消息边界语义：利用一个定长数据来保存整个请求协议体的大小；先读取固定长度的位置里面的值，得到协议体长度，再去读取整个协议体的数据

    ![image-20210522162421147](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522162421147.png)

- 协议数据序列化方法信息：利用定长的位置存储协议数据的序列化方式

- 将整个协议分为协议头和协议体，得到定长协议头，该协议头是不可扩展的

    ![image-20210522162430292](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522162430292.png)

- 可扩展协议，将协议头改为可扩展的。将协议分为三部分：固定部分，协议头内容，协议体内容；前两部分统称为协议头

    ![image-20210522162827231](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522162827231.png)

- RPC为了吞吐量，都是异步并发发送的请求，等待服务应答，因此需要消息ID，来判断应答对应哪个请求




**golang 实现rpc序列化**

RPC客户端调用如下:
`err = client.Call("service.Method",args,&reply)`
客户端发送的请求有包含服务名,方法名,参数列表
服务端返回的响应有错误,返回值
将请求和响应中的参数和返回值抽象为body,那么剩余的信息可以抽象为一个Header

```go
type Header struct {
	ServiceMethod string //服务名和方法名
	Seq uint64 //请求序号
	Error string //客户端为空,服务端如果发生错误,会把错误信息放到Error中
}
```

### RPC网络通信

**常见的网络IO模型**

- 同步阻塞 IO（BIO）
    - 在 Linux 中，默认情况下所有的 socket 都是 blocking 的
    - 应用进程发起 IO 系统调用后，应用进程被阻塞，转到内核空间处理。之后，内核开始等待数据，等待到数据之后，再将内核中的数据拷贝到用户内存中，整个 IO 处理完毕后返回进程。最后应用的进程解除阻塞状态，运行业务逻辑。
    - ![image-20210522185621450](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522185621450.png)
    - 系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。而在这两个阶段中，应用进程中 IO 操作的线程会一直都处于阻塞状态，如果是基于 Java 多线程开发，那么每一个 IO 操作都要占用线程，直至 IO 操作结束。
    - 阻塞 IO 每处理一个 socket 的 IO 请求都会阻塞进程（线程），但使用难度较低。在并发量较低、业务逻辑只需要同步进行 IO 操作的场景下，阻塞 IO 已经满足了需求，并且不需要发起 select 调用，开销上还要比 IO 多路复用低。
- 同步非阻塞 IO（NIO）
- 同步IO 多路复用（select，poll，epoll）
    - 多路复用 IO 是在高并发场景中使用最为广泛的一种 IO 模型
    - linux总的多个网络连接的 IO 可以注册到一个复用器（select）上，当用户进程调用了 select，那么整个进程会被阻塞。同时，内核会“监视”所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从内核中拷贝到用户进程。
    - 优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求。用户可以注册多个 socket，然后不断地调用 select 读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。
    - IO 多路复用更适合高并发的场景，可以用较少的进程（线程）处理较多的 socket 的 IO 请求。
- 异步非阻塞 IO（AIO）

**RPC网络io模型**

RPC 调用在大多数的情况下，是一个高并发调用的场景

- 在 RPC 框架的实现中，在网络通信的处理上，我们会选择 IO 多路复用的方式。

- 选择基于 Reactor 模式实现的io框架来实现IO多路复用

- 在 Linux 环境下，也要开启 epoll 来提升系统性能（Windows 环境下是无法开启 epoll 的，因为系统内核不支持）。

**网络io中的零拷贝**

系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。

- 等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中
- 拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中。

应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA 将这份数据拷贝到网卡中，最后由网卡发送出去。一次写操作数据要拷贝两次才能通过网卡发送出去

![image-20210522192234064](https://gitee.com/Euraxluo/images/raw/master/picgo/image-20210522192234064.png)

- 零拷贝技术
    - 零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作，应用进程每一次的读写操作，都可以通过一种方式，让应用进程向用户空间写入或者读取数据，就如同直接向内核空间写入或者读取数据一样，再通过 DMA 将内核中的数据拷贝到网卡，或将网卡中的数据 copy 到内核。
- 零拷贝实现
    - mmap+write 方式，核心原理是通过虚拟内存来解决的
    - sendfile 方式
- Netty零拷贝实现：
    - 用户空间数据操作零拷贝优化
        - 收到数据包后，在对数据包进行处理时，需要根据协议，处理数据包，在进行处理时，免不了需要进行在用户空间内部内存中进行拷贝处理，Netty就是在用户空间中对数据操作进行优化
        - Netty 提供了 CompositeByteBuf 类，它可以将多个 ByteBuf 合并为一个逻辑上的  ByteBuf，避免了各个 ByteBuf 之间的拷贝。
        - ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝。
        - 通过 wrap 操作，我们可以将 byte[] 数组、ByteBuf、ByteBuffer  等包装成一个 Netty ByteBuf 对象, 进而避免拷贝操作。
    - 用户空间与内核空间之间零拷贝优化
        - Netty  的  ByteBuffer 可以采用 Direct Buffers，使用堆外直接内存进行 Socket 的读写操作，效果和虚拟内存所实现的效果是一样的。
        - Netty  还提供  FileRegion  中包装  NIO  的  FileChannel.transferTo()  方法实现了零拷贝，这与 Linux  中的  sendfile  方式在原理上也是一样的。

### RPC框架设计：

#### 屏蔽处理流程

- java使用动态代理屏蔽实现细节
- golang使用反射等，来实现的

#### RPC架构

##### 网络传输模块

用于收发二进制数据

##### 协议模块

保证数据在网络中正确传输，包括序列化和反序列化功能，数据压缩功能，以及通信协议约定

##### Bootstrap模块

用于屏蔽RPC细节，利用反射或者代理让远程调用大大简化

##### 服务治理模块

赋予RPC服务集群能力，包括服务注册和发现，负载均衡，连接管理，路由，容错和配置管理

架构图如下：

![](https://gitee.com/Euraxluo/images/raw/master/picgo/30f52b433aa5f103114a8420c6f829fb.jpg)

##### 利用微内核架构，将组件插件化

将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离并提供接口的默认实现。

提升了RPC框架的可扩展性，实现了开闭原则，用户可以非常方便地通过插件扩展实现自己的功能，而且不需要修改核心功能的本身；其次保持了核心包的精简，依赖外部包少，这样可以有效减少开发人员引入 RPC 导致的包版本冲突问题。

![](https://gitee.com/Euraxluo/images/raw/master/picgo/a3688580dccd3053fac8c0178cef4ba6.jpg)



### 服务注册与发现：

#### 概述：

服务发现（Service Discoery）要解决的是分布式系统中最常见的问题之一，即在同一个分布式系统中的进程如何才能找到对方并建立连接。

##### 服务发现组件的需求

服务发现组件需要以下一些功能：

- 怎么标识一个服务

- 根据服务名的得到服务可用列表
- 服务注册功能，因此高组件是一个独立的，简单的第三方存储，并且存储极简化。
- 同时服务发现组件还需要有服务探活功能，应当提供很多的服务探活选项

##### 服务代理

服务发现组件的从需求上看是服务代理，常见对的服务代理有：

1. 网络代理
    - 如果是使用http通信，那么可以使用nginx作为反向代理，转到各个服务
    - 如果是RPC服务则可以使用LVS或者ESB之内的网络代理服务地址
    - 缺点：当服务增多是，需要维护超多的网络代理，最后将陷入到运维灾难中
2. DNS方式
    - 为服务A配置域名，然后通过配置两个分别指向服务A的实例，客户端只需要使用配置A的域名就可以
    - 问题：DNS是IP级别，无法处理端口信息，DNS携带的数据较少，节点权重，序列化信息等数据无法传递。

服务代理无法满足服务发现组件对的所有需求，所以需要找另外的组件。

#### zookeeper做为服务发现的问题

Zookeeper旨在解决大规模分布式应用场景下的服务协调同步问题；他可以为同在一个分布式系统中的其他服务提供：统一命名服务，配置管理服务，分布式锁服务，集群管理服务等。

##### CAP（C-数据一致性；A-服务可用性；P-服务对网络分区故障的容错性）

zk是一个CP的，即咋子任何时候对于ZK的访问请求都能得到一致的是数据结果，同时系统对于网络分割具备容错性。

##### ZK解决的问题

Zk是一个分布式协调服务，他被设计用于保证数据在其管辖下，在所有服务之前保持同步，一致，因此ZK被设计为CP的。

##### ZK作为服务发现服务的问题

由于zk不能保证每次服务的可用性：

1. 因为对于服务发现服务来说，宁可返回某个包含了不实信息的结果也比什么都不返回的好。
2. 宁可返回某服务5分钟之前在在某几台服务器上可用的信息，也不能因为暂时的网络故障找不到可用的服务器。
3. ZK中，若某网络分区中的节点数小于ZK选取leader节点的法定人数，那么这些节点将会断开，就无法正确提供服务了
4. ZK的特点是强一致性，所有导致ZK集群的每个节点数据在发生更新时，需要通知其他ZK节点同时执行更新，所以当大量服务节点上线时，可能会导致ZK集群无法承载

##### 局限性:

1. 网络化分后，强一致性导致服务注册机制会失效

    ZAB协议保证数据一致性，当发生网络分割时，会破坏服务的整体联通性

2. 持久化存储和事务日志

    为了保证数据一致性，zk使用的事务日志，当集群半数节点写入成功时，该事务有效。同时事务写使用的2PC提交的方式

    但是注册中心只关心实时的健康服务列表，因为调用方不关心历史服务和状态

3. 服务探活

    ZK注册镇中心通常利用session活性心跳和临时节点机制进行服务探活

    将服务的健康检查检测绑定在了ZK对于Session的健康监测上。然后其实应该由服务方决定探活方式

4. 服务容灾

    服务调用链路弱依赖注册中心，同时ZK客户端并无客户端缓存机制

##### 改良

1. 加上服务可用性。使用客户端缓存，当部分节点与zk断开时，每个节点依然能从本地缓存中获取到数据，但是ZK不能保证所有节点任何时刻都能缓存所有的服务注册信息。
2. 将ZK的强一致性改为Ap并保证最终一致性：当我们需要最终一致性时，可以使用消息总线机制。注册数据可以全量缓存在每个注册中心内存中。通过消息总线同步数据。当有一个节点接收到服务节点注册时，会产生一个消息推送到消息总线，最后再通过消息总线通知给其他的注册中心节点更新数据，并进行服务下发，从而达到注册中心数据的最终一致性。

#### Eureka:专为服务发现设计对的开源组件

Eureka由Eureka服务器和Eureka客户端组成，Eureka服务器作为服务注册服务器，Eureka客户端是一个java客户端，用于简化与服务器的操作，作为轮询负载均衡器，并提供服务的故障切换支持。

##### Eureka Server：注册中心服务端

注册中心服务端主要提供了三个功能

**服务注册**

服务提供者启动后，会通过Eureka Client 向Eureka Server注册信息，Eureka Server会存储该服务的信息，Eureka Server内部有二层缓存机制来维护整个注册表

**提供注册表**

服务消费者在调用服务时，如果Eureka Client 没有缓存注册表的话，会从Eureka Server 获取最新的注册表

**同步状态**

Eureka Client 通过注册，心跳机制和Eureka Server 同步当前客户端的状态

##### Eureka Client：注册中心客户端

Eureka Client是一个java客户端，用于简化与Eureka Server的交互，Eureka Client会拉取，更新和缓存Eureka Server中的信息。因此当所有的Eureka Server节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者，但是当服务更改时会出现信息不一致

**Registry：服务注册**

服务的提供者，将自身注册到注册中心，服务提供者也是一个Eureka Client。当Eureka Client向Eureka Server注册时，它提供自身的元数据。

**Renew：服务续约**

Eureka Client会每间隔30s发送一次心跳进行续约。如果续约来告知Eureka Server该Eureka Client运行正常，没有正常问题。默认情况下，如果Eureka Server在90s内没有收到Eureka Client的续约，Server端就会将实例从注册表中删除。

**Eviction：服务剔除**

当Eureka Client和Eureka Server不在有心跳时，Eureka Server会从该服务实例从服务注册列表中删除，即服务剔除

**Cancel：服务下线**

Eureka Client在程序关闭时向Eureka Server发送取消请求。发送请求后，该客户端实例信息将从Eureka Server的实力注册表中删除。该下线请求不会自动完成，需要调用特殊的方法

**GetRegistry：获取注册列表信息**

Eureka Client从服务器获取注册表信息，并将其缓存在本地。客户端会使用该信息查找其他服务，从而进行远程调用。该注册列表信息会定期清理一次。重新拉取新的注册表信息

**Remote Call：远程调用**

当Eureka Client从注册中心获取到1服务提供者信息后，就可以通过Http请求调用对应的服务了；服务提供者有多个时，Eureka Client客户端会通过Ribbon进行自动负载均衡。


##### 高可用：

在Eureka平台中，若某台服务器宕机，Eureka服务不会有类似Zookeeper选举的过程，客户端会自动切换到新的Eureka节点；当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理中；因此不用担心会有服务器从集群中剔除的风险

##### 应对网络分割故障

当网络分割故障出现时，每个Eureka节点会持续的对外服务，接收新的服务注册请求同时将他们提供给下游的服务发现请求。这样在一个子网中，新发布的服务依然可以被发现与访问

##### 节点自我保护

Eureka内置了心跳服务，用于淘汰一些假死的服务器；如果在Eureka中注册的服务，心跳变的迟缓，Eureka会将其整个剔除出管理范围。这个功能在发生网路分割故障时会很危险。因为可能服务器是正常的，只不过是因为网络问题到了一个子网中

Netflix考虑添加了自我保护机制,如果Eureka服务节点在短时间内丢失了大量心跳连接，那么该服务节点会进入自我保护状态，这些节点的服务注册信息将不会过期，即便是假死状态，以防还有客户端会向该假死节点发起请求。同时当Eureka节点恢复后，会退出自我保护模式

##### 客户端缓存

Eureka最后还有客户端缓存的功能。当所有的Eureka集群节点都失效，或者发生网络分割故障导致客户端不能访问任何一台Eureka服务器。Eureka消费者依然可以通过客户端缓存找到现有对的服务注册信息



#### etcd 工作原理

etcd是CoreOS团队于2013年发起的开源项目，目标是构建一个高可用的分布式键值数据库，etcd内部采用raft协议作为一致性算法，并给予Golang语言实现

##### 架构

- 网络层:提供网络数据读写功能，监听服务端口，完成集群节点之间数据通信，收发客户端数据。
- Raft模块: Raft强一致性算法的具体实现。
- 存储模块:涉及KV存储、WAL文件、Snapshot管理等，用于处理etcd支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等，是etcd对用户提供的大多数API功能的具体实现。
- 复制状态机:这是一个抽象的模块，状态机的数据维护在内存中，定期持久化到磁盘，每次写请求都会持久化到WAL文件，并根据写请求的内容修改状态机数据。除了在内存中存有所有数据的状态以及节点的索引之外，etcd还通过WAL进行持久化存储。基于WAL的存储系统其特点就是所有的数据在提交之前都会事先记录日志。Snapshot是为了防止数据过多而进行的状态快照。

##### 基本功能

1. KV存储

    可以用于存储数据，也就是可以存储配置数据

2. watch功能

    可以发现配置的异动，在配置变化时可以起到通知作用

3. key TTL功能

    有key TTL功能，etcdv3可以通过lease租约，设置服务的生存周期，然后通过KeepAlive定期续租，避免过期

##### 服务注册原理

在etcd中服务的注册使用租约（lease）来实现，设置租约的时候按需求设置租约的时间（ttl），类似redis中的EXPIRE key，再把服务自身的节点元数据写入对应的key中，定时去调用KeepAliveOnce来保持租约，如果在期间KeepAliveOnce的消息丢失，或者延迟大于这个租约的ttl则etcd中即将会把这个节点的信息删除，恢复正常时重新发起租约流程

##### 存储结构

etcd3没有树的概念，因此我们需要将平铺展开来的键值对抽象为树的概念，和其他类型的注册中心保持一致

例如：接口子目录,这些都是不过期的节点

```bash
/root/interface/providers
/root/interface/consumers
/root/interface/routers
/root/interface/configurators
```

临时节点存储：

```bash
/root/interface/providers/protocol:ip:port/service?xxx=xxx
/root/interface/consumers/protocol:ip:port/service?xxx=xxx
/root/interface/routers/action:ip:port/service?xxx=xxx
/root/interface/configurators/protocol:ip:port/service?xxx=xxx
```

##### 优点

1. etcd 使用增量快照 ， 可以避免在创建快照时暂停 。
2. etcd 使用堆外存储 ， 没有垃圾收集暂停功能 。
3. etcd 己经在微服务 Kubernates 领域中有大量生产实践 ， 其稳定性经得起考验 。
4. 基于 etcd 实现服务发现时 ， 不需要每次感知服务进行全量拉取 ， 降低了网络冲击 。
5. etcd 具备更简单的运维和使用特性 ， 基于 Go 开发更轻量 。
6. etcd 的 watch 可以一直存在 。
7. ZooKeeper 会丢失一些旧的事件 ， etcd 设计了一个滑动窗口来保存一段时间内的事件 ， 客户端重新连接上就不会丢失事件了 。
8. etcd支持多语言客户端

##### 临时节点的创建

注意点：

- 防御性容错，允许失败充实，默认策略最多重试一次，每次重试休眠1秒

流程

1. 检查client是否正确初始化，只有正确初始化才会触发后续
2. 创建租约并进行保活，将用户配置的session作为keep-alive时间，默认30s
3. 创建key-value并绑定租约，通过两个线程，一个定期刷新TTL保活，第二个定期检测本地是否过期

##### 获取子节点

注意点

- 因为etcd是平铺的key-value。这里为了避免每次单个provider上线都会触发所有客户端进行拉取，所以使用元数据作为key，并且使用特定的前缀进行区分

流程

​		若我们将获取服务为xxx的providers，这里path对应`xxx/providers`，我们需要将初始索引移动到path的最后一个`/`字符,也就是xxx后第一个字符。通过这个机制来获取我们需要的节点key

##### 删除子节点

流程

- 通过kvClient直接删除对应的path即可

##### 接收watch事件变更

流程

1. 获取服务端事件推送

    从gRPC响应中获取响应事件

2. 根据event.type做不同的处理

    先判断是否为当前服务的path

3. PUT

    新服务上线，动态配置，或者动态路由的下发，直接将服务的元数据保存在URL中

4. DELETE

    服务下线或者动态配置，动态路由的删除

    直接将元数据从URL中删除

##### watch请求监听

注意点

- 幂等，对一个path多次watch时，会取消之前的watch
- watch的丢失和取消，以及自动重试，应该保证可用性，否则会影响服务订阅

流程：

1. 幂等处理
2. 创建gRPC远程本地调用的代理
3. 创建watch对象，并关联回调函数
4. 发起gRPC watch调用
5. 首次监听时，先手动拉取全部的节点数据


##### 保证一致性：

etcd 使用raft协议来维护集群内各个节点状态的一致性。etcd集群是一个分布式系统，由多个节点相互通信构成一个整体对外服务，每个节点都存储了完整的数据，并且通过Rat协议保证每个节点维护的数据是一致的。

每个etcd节点都维护了一个状态机，并且任意时刻至多存在一个有效的主节点，主节点处理所有来自客户端的写操作，通过raft协议保证写操作对状态机的改动会可靠的同步到其他节点

##### 高性能

单实例支持每秒一千次以上的写从操作，极限写性能可达10kQPS

##### 安全

支持TLS客户端安全认证



### 健康监测

健康监测的目标是为了让调用方可以感知到节点的状态变化

##### 心跳机制

服务调用方，每隔一段时间就询问服务提供方，节点的状态

状态：

- 健康状态：建立连接成功，并且心跳探活成功
- 亚健康状态：建立连接成功，但是心跳请求连续失败
- 死亡状态：建立连接失败

##### 业务优化

健康监测最终目的还是希望可以让某些不健康的节点不要影响我们的业务。因此可以再健康监测时，加入业务相关的因素，例如可用率`时间窗内接口调用成功次数/时间窗内总调用次数`。对于该类低于预期的节点，可以加入到亚健康状态中。

##### 其他考虑

- 健康监测程序所在机器和节点所在机器的网络依然可能故障，也就会出现误判。这时可以将检测程序部署再不同的机器和机房中
- 可以将服务的返回状态保存在MQ中，然后使用专门的消费者进行消费，如果失败率大于阈值，就调用注册中心，进行下线。



### 路由策略

路由策略的目标是希望使用合理的路由策略，1.：选出合适的服务节点子集；2.：让服务的变更平滑过渡。

在一般的集群中，如果采用灰度发布，但是服务如果出问题，影响范围依然不可控，特别是基础服务，影响范围会变得很大。

路由策略就是通过改变调用请求的路由方向，细粒度的控制服务的影响范围。

##### 路由策略位置

调用方发起RPC调用流程：

1. 首先服务发现会返回可用的服务列表
2. 在可用服务列表中选择合适的节点发起请求

我们就可以在第二部，**在可用的服务列表中选择合适的节点**这个流程，加上合适的节点筛选规则，这个规则就是路由策略。

最后，流程图如下：

![](https://gitee.com/Euraxluo/images/raw/master/picgo/b78964a2db3adc8080364e9cfc79ca68.jpg)

##### IP路由

IP路由策略可以限制调用服务方的IP，使用了ip路由策略后，对于服务变更，我们就可以将服务调用方的ip做限制，让变更的节点，只被少数调用方使用。

##### 参数路由

首先，将每次变更的节点打上tag，例如version等，用于区分不同批次的节点。然后我们需要一个参数配置中心。用于配置我们的参数路由策略。

然后当我们进行请求时，就可以根据请求参数，根据参数规则过滤响应的节点，然后就实现了我们的参数路由策略。

##### 路由功能的用途

- 灰度发布
- 定点调用
- 黑白名单
- ab_test
- 并行开发时，隔离出不同的环境

#### dubbo的路由功能

dubbo主要是服务路由，服务路由包含一条路由规则，路由规则决定了服务消费者的调用目标，即规定了服务消费者可以调用哪些服务提供者

##### buddo的路由策略

- 条件路由ConditionRouter

    将条件规则配置成kv对，然后对条件规则配置进行解析，路由时，将路由到符合条件host中

- 脚本路由ScriptRouter

    将脚本作为字符串传入脚本路由解析器中，通过脚本的类型，调用对应的脚本解析器，然后将数据传入至脚本中，运行结束后得到应该路由的host

- 标签路由TagRouter

    对于服务配置tag标签对，例如tag_name1=>host1,加载tag配置后，解析为标签路由配置项，会将tag路由到对应的host中

##### dubbo的路由创建时机

每次url发生变更后，都会触发路由信息重建

### 负载均衡

负载均衡SLB是一种对流量进行按需分发的服务，通过将流量分发到不同的后端服务来扩展应用系统的吞吐能力，并且可以消除系统中的单调故障，提升系统的可用性

负载均衡主要分为应用型负载均衡和传统型负载均衡

应用型负载均衡主要面向七层，基于负载均衡应用

传统型负载均衡主要面向四层，基于物理机架构

##### RPC的负载均衡

RPC的负载均衡完全由RPC框架自身实现，RPC的服务调用在每次发起RPC调用时，服务调用者都会根据负载均衡插件

RPC负载均衡策略一般是包括权重，随机权重，一致性Hash，轮询，随机。

##### 轮询法

- 获取地址列表，并维护一个地址指针，每次循环取指针指向的地址，当指针大于地址列表长度时，重置为0
- 访问次数%地址列表长度，注意访问次数使用原子类计数器实现

##### 权重法

例如： 

address1 weight 1

address2 weight 2

地址列表：address1 address2 address3

- 对地址列表中的地址，根据权重，重复，例如address2，权重为2，则重复两次。将这样的结果作为地址列表，再进行轮询
- **随机权重**，在得到根据权重修改的列表后，根据随机法获取地址

##### 一致性hash：相同的参数总是落在一个节点上

- hash(参数)%地址列表长度，得到这样的index，然后从地址列表中获取对应的数据

##### 最少活跃调用数

- 相同活跃数的随机，活跃数值得是调用前后技术差

##### 自适应的负载均衡

负载均衡插件需要得到每一个服务节点的处理请求的能力，然后根据处理能力来分配流量。

服务调用者在与服务节点进行长连接时，可以手机服务节点的各个指标，例如：CPU核数，请求处理的耗时指标情况(请求平均耗时，TP99)，内存大小,服务节点的健康状态。然后根据很多指标，并根据每个指标的权重，得到一个总体的数据。

最后得到每个节点的分数后，根据最终的指标分数修改服务节点的最终权重，然后再使用随机权重法来进行流量调度

![](https://gitee.com/Euraxluo/images/raw/master/picgo/00065674063f30c98caaa58bb4cd7baf.jpg)

**步骤**

1. 添加服务指标收集器，并将其作为插件，可以在运行时收集状态指标，默认有健康状态收集，请求耗时收集等
2. 运行时状态指标收集器收集服务节点的基本数据和健康状态，在服务调用者和服务提供者的心跳数据中获取
3. 请求耗时指标收集器收集请求耗时指标，例如平均耗时，TP99等
4. 配置指标收集器的开启，并且可以设置这些参考指标的指标权重，再根据指标数据和指标权重来综合打分
5. 根据服务节点的综合打分和节点的权重，最终得到节点的最终权重，之后服务调用者再根据随机权重法选择服务节点

#### dubbo负载均衡

##### 多种级别

- 服务端级别
- 服务端方法级别
- 客户端服务级别
- 客户端方法级别

dubbo的多种配置是有覆盖关系的，配置优先级是

1. 客户端方法级别
2. 客户端接口级别
3. 服务端方法级别
4. 服务端接口级别

### 异常重试

RPC的重试机制：当服务调用端发起RPC调用时，会经过负载均衡，选择一个节点，之后会向该节点发起请求。当消息发送失败或者收到异常消息时，我们1就可以捕获异常，当异常符合条件，根据异常触发重试，重新通过负载均衡选一个新的节点请求消息，并且记录重试次数，当次数达到阈值，就返回给调用端一个失败异常。

##### 幂等

在使用RPC框架时，我们要确保被调用的服务的业务逻辑是幂等的，这样才能根据开启RPC的异常重试。

##### 注意点

- 当连续重试时，请求超时时间需要每次重试后都进行重置，否则会超出用户设置的超时时间
- 在发起重试，负载均衡选择节点时，应该去掉之前异常的节点，保证重试的成功率
- 异常重试白名单，当网络异常，连接异常等一些异常我们知道需要进行重试，但是由很多业务异常也是需要进行异常重试的。这时我们需要配置一个异常重试白名单。当捕获异常后，如果异常在白名单中，我们就需要对这个请求进行重试

#### dubbo集群容错

dubbo的集群容错功能由多个组件共同完成：包括Cluster，Cluster Invoker，Directory，Router，LoadBalance

![](https://gitee.com/Euraxluo/images/raw/master/picgo/830731-20200502203856324-835993574.jpg)

- Failover Cluster:失败自动恢复

    不断重试机制，会把请求过得节点保存进来，避免重复请求

- Failfast Cluster：快速失败

    只调用一次，异常则抛出，正常则返回结果

- Failsafe Cluster：失败安全

    只调用一次，异常时，忽略所有异常，返回默认值，正常则返回结果。

- Failback Cluster：失败自动恢复

    只调用一次，当调用失败后，会将失败的请求，放入到延迟队列中，等待一会之后，重试。

- Forking Cluster：并行调用多个服务提供者

    同时发起n个并发请求调用者，返回最先响应的结果，其他忽略。若都失败，则返回自定义异常

- Broadcast：-广播容错

    向所有invoker发起调用，全部成功才算成功

- mergeable：归并容错

    调用所有的invoker，最后merge所有的返回结果

### 优雅关闭









